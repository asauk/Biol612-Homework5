---
title: "Homework 5 A.Sauk"
author: "Alexandra Sauk"
date: "08/03/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rpart)
library(rpart.plot)

mydata<-read.csv("crimedata.csv")

summary(mydata)
```

## Q2 - Building the regression tree
```{r}
#Building regression tree with crimedata
m.rpart <- rpart(CrimeRate ~ ExpenditureYear+StateSize+BelowWage+Education+YouthUnemployment+Wage, data = mydata) 

```

##Q3 Summary of regression tree

```{r}
#Summary of regression tree
summary(m.rpart)
```

The most important variables in the regression tree are the expenditure per capita on police (ExpenditureYear), the median weekly wage (Wage), and number of families below half wage per 1000 (BelowWage).

## Q4 Plot the regression tree
```{r}
rpart.plot(m.rpart, digits = 3, fallen.leaves = TRUE,tweak=1.3)

printcp(m.rpart)
```

From the regression tree, 51.1% of the states have police expenditures being greater than 77 per capita with 27.7% having police expenditures between 77 and 108 per capita while 23.4% have police expenditures about 108 per capita. For those states with police expenditures below 77 per capita, 25.5% of them have a state size of less than 23 000 000 whereas 23.4% have a state size of greater than 23 000 000.

## Q5 Mean crime rates

Mean crime rates for each group are:  
Expenditure year < 77 and state size < 23 : 72.5 offences per million people  
Expenditure year < 77 and state size > 23 : 97.6 offences per million people  
Expenditure year > 108 : 131 offences per million people  
Expenditure year > 77 < 108 : 111 offences per million people  

## Q6 excluded variables

Yes, four of the six variables (BelowWage, Education, YouthUnemployment, and Wage) were excluded by the rpart function. The rpart function did not include these variables because including them would not have sufficiently decreased the complexity parameter and relative error values for the tree.

## Q7 using model to predict crime data ten years later

```{r}
data10<-read.csv("crimedata10.csv")
p.rpart <- predict(m.rpart, data10)
```

##Q8 Pearson Correlation

```{r}
cor(p.rpart, data10[["CrimeRate"]],method="pearson")
```

The correlation coefficient is 0.58. 

##Q9 Mean absolute error

```{r}
MAE <- function(actual, predicted)  {
  mean(abs(actual - predicted))
}

MAE(predicted = p.rpart,actual = mydata[["CrimeRate"]])
```

The mean absolute error for the model is 18.37 offences per million people and given the correlation coefficient of 0.58, I do not think the model was very good at predicting crime rates.

##Q10 Null distribution

```{r}
crimedata=mydata[["CrimeRate"]]

MAE2 <- function(data,indices)  {
  d<-data[indices]
  return(mean(abs(crimedata - d)))
}

library(boot)
guesses=boot(data=crimedata, statistic=MAE2, R=1000)

{hist(guesses$t)
abline(v=mean(guesses$t),col="red")}
mean(guesses$t)
```

##Q11 Mean absolute error comparison

The mean absolute error when crime rates are randomly assigned is 32.6 offences per million people which is worse than the mean absolute error of the model. This suggests that the model appears to be better at predicting crime rates than random. 

##Q12 P value

```{r}
p.value=length(which((guesses$t<0.5198)==T))/1000
p.value
```

The p-value is less than 0.05 so the model is significantly better at predicting crime rates than random chance. 
